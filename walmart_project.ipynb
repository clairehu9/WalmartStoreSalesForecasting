{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"TripType\",\"VisitNumber\",\"Weekday\",\"Upc\",\"ScanCount\",\"DepartmentDescription\",\"FinelineNumber\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The jupyter notebook for Walmart project\n",
    "# Claire Hu\n",
    "#\n",
    "\n",
    "input_file = 'data/train.csv'\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    title_str = f.readline()\n",
    "\n",
    "print(title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TripType': 0, 'VisitNumber': 1, 'Weekday': 2, 'Upc': 3, 'ScanCount': 4, 'DepartmentDescription': 5, 'FinelineNumber': 6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "title_layout = {}\n",
    "index = 0\n",
    "for item in title_str.strip('\\n').split(','):\n",
    "    title_layout[item.strip('\"')] = index\n",
    "    index += 1\n",
    "print(title_layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DepartmentDescription_set = set()\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if 'TripType' not in line:\n",
    "            line_list = line.strip('\\n').split(',')\n",
    "            DepartmentDescription = line_list[title_layout['DepartmentDescription']].strip('\"')\n",
    "            DepartmentDescription_set.add(DepartmentDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique DepartmentDescription is: 70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total unique DepartmentDescription is:', len(DepartmentDescription_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique Weekday list is: {'Wednesday', 'Tuesday', 'Monday', 'Friday', 'Thursday', 'Sunday', 'Saturday'} 7\n"
     ]
    }
   ],
   "source": [
    "Weekday_set = set()\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if 'TripType' not in line:\n",
    "            line_list = line.strip('\\n').split(',')\n",
    "            Weekday = line_list[title_layout['Weekday']].strip('\"')\n",
    "            Weekday_set.add(Weekday)\n",
    "\n",
    "print('Total unique Weekday list is:', Weekday_set, len(Weekday_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed list is:\n",
      "['TripType', 'VisitNumber', 'Weekday', 'Upc', '1-HR PHOTO', 'ACCESSORIES', 'AUTOMOTIVE', 'BAKERY', 'BATH AND SHOWER', 'BEAUTY', 'BEDDING', 'BOOKS AND MAGAZINES', 'BOYS WEAR', 'BRAS & SHAPEWEAR', 'CAMERAS AND SUPPLIES', 'CANDY', 'CELEBRATION', 'COMM BREAD', 'CONCEPT STORES', 'COOK AND DINE', 'DAIRY', 'DSD GROCERY', 'DepartmentDescription', 'ELECTRONICS', 'FABRICS AND CRAFTS', 'FINANCIAL SERVICES', 'FROZEN FOODS', 'FURNITURE', 'GIRLS WEAR', 'GROCERY DRY GOODS', 'HARDWARE', 'HEALTH AND BEAUTY AIDS', 'HOME DECOR', 'HOME MANAGEMENT', 'HORTICULTURE AND ACCESS', 'HOUSEHOLD CHEMICALS/SUPP', 'HOUSEHOLD PAPER GOODS', 'IMPULSE MERCHANDISE', 'INFANT APPAREL', 'INFANT CONSUMABLE HARDLINES', 'JEWELRY AND SUNGLASSES', 'LADIES SOCKS', 'LADIESWEAR', 'LARGE HOUSEHOLD GOODS', 'LAWN AND GARDEN', 'LIQUOR', 'MEAT - FRESH & FROZEN', 'MEDIA AND GAMING', 'MENS WEAR', 'MENSWEAR', 'NULL', 'OFFICE SUPPLIES', 'OPTICAL - FRAMES', 'OPTICAL - LENSES', 'OTHER DEPARTMENTS', 'PAINT AND ACCESSORIES', 'PERSONAL CARE', 'PETS AND SUPPLIES', 'PHARMACY OTC', 'PHARMACY RX', 'PLAYERS AND ELECTRONICS', 'PLUS AND MATERNITY', 'PRE PACKED DELI', 'PRODUCE', 'SEAFOOD', 'SEASONAL', 'SERVICE DELI', 'SHEER HOSIERY', 'SHOES', 'SLEEPWEAR/FOUNDATIONS', 'SPORTING GOODS', 'SWIMWEAR/OUTERWEAR', 'TOYS', 'WIRELESS']\n"
     ]
    }
   ],
   "source": [
    "transformed_title_list = [\"TripType\", \"VisitNumber\", \"Weekday\", \"Upc\"]\n",
    "for field in sorted(DepartmentDescription_set):\n",
    "    transformed_title_list.append(field)\n",
    "\n",
    "print('The transformed list is:')\n",
    "print(transformed_title_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TripType': 0, 'VisitNumber': 1, 'Weekday': 2, 'Upc': 3, '1-HR PHOTO': 4, 'ACCESSORIES': 5, 'AUTOMOTIVE': 6, 'BAKERY': 7, 'BATH AND SHOWER': 8, 'BEAUTY': 9, 'BEDDING': 10, 'BOOKS AND MAGAZINES': 11, 'BOYS WEAR': 12, 'BRAS & SHAPEWEAR': 13, 'CAMERAS AND SUPPLIES': 14, 'CANDY': 15, 'CELEBRATION': 16, 'COMM BREAD': 17, 'CONCEPT STORES': 18, 'COOK AND DINE': 19, 'DAIRY': 20, 'DSD GROCERY': 21, 'DepartmentDescription': 22, 'ELECTRONICS': 23, 'FABRICS AND CRAFTS': 24, 'FINANCIAL SERVICES': 25, 'FROZEN FOODS': 26, 'FURNITURE': 27, 'GIRLS WEAR': 28, 'GROCERY DRY GOODS': 29, 'HARDWARE': 30, 'HEALTH AND BEAUTY AIDS': 31, 'HOME DECOR': 32, 'HOME MANAGEMENT': 33, 'HORTICULTURE AND ACCESS': 34, 'HOUSEHOLD CHEMICALS/SUPP': 35, 'HOUSEHOLD PAPER GOODS': 36, 'IMPULSE MERCHANDISE': 37, 'INFANT APPAREL': 38, 'INFANT CONSUMABLE HARDLINES': 39, 'JEWELRY AND SUNGLASSES': 40, 'LADIES SOCKS': 41, 'LADIESWEAR': 42, 'LARGE HOUSEHOLD GOODS': 43, 'LAWN AND GARDEN': 44, 'LIQUOR': 45, 'MEAT - FRESH & FROZEN': 46, 'MEDIA AND GAMING': 47, 'MENS WEAR': 48, 'MENSWEAR': 49, 'NULL': 50, 'OFFICE SUPPLIES': 51, 'OPTICAL - FRAMES': 52, 'OPTICAL - LENSES': 53, 'OTHER DEPARTMENTS': 54, 'PAINT AND ACCESSORIES': 55, 'PERSONAL CARE': 56, 'PETS AND SUPPLIES': 57, 'PHARMACY OTC': 58, 'PHARMACY RX': 59, 'PLAYERS AND ELECTRONICS': 60, 'PLUS AND MATERNITY': 61, 'PRE PACKED DELI': 62, 'PRODUCE': 63, 'SEAFOOD': 64, 'SEASONAL': 65, 'SERVICE DELI': 66, 'SHEER HOSIERY': 67, 'SHOES': 68, 'SLEEPWEAR/FOUNDATIONS': 69, 'SPORTING GOODS': 70, 'SWIMWEAR/OUTERWEAR': 71, 'TOYS': 72, 'WIRELESS': 73}\n"
     ]
    }
   ],
   "source": [
    "transformed_title_layout = {}\n",
    "index_t = 0\n",
    "for item in transformed_title_list:\n",
    "    transformed_title_layout[item] = index_t\n",
    "    index_t += 1\n",
    "print(transformed_title_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique TripType list is: {'8', '15', '29', '39', '38', '35', '43', '12', '36', '3', '4', '42', '31', '24', '7', '25', '21', '28', '32', '34', '14', '44', '26', '27', '999', '40', '6', '5', '30', '18', '22', '20', '33', '23', '9', '41', '19', '37'} 38\n"
     ]
    }
   ],
   "source": [
    "TripType_set = set()\n",
    "with open(input_file, 'r') as f:\n",
    "    for line in f:\n",
    "        if 'TripType' not in line:\n",
    "            line_list = line.strip('\\n').split(',')\n",
    "            TripType = line_list[title_layout['TripType']].strip('\"')\n",
    "            TripType_set.add(TripType)\n",
    "\n",
    "print('Total unique TripType list is:', TripType_set, len(TripType_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'12': '0', '14': '1', '15': '2', '18': '3', '19': '4', '20': '5', '21': '6', '22': '7', '23': '8', '24': '9', '25': '10', '26': '11', '27': '12', '28': '13', '29': '14', '3': '15', '30': '16', '31': '17', '32': '18', '33': '19', '34': '20', '35': '21', '36': '22', '37': '23', '38': '24', '39': '25', '4': '26', '40': '27', '41': '28', '42': '29', '43': '30', '44': '31', '5': '32', '6': '33', '7': '34', '8': '35', '9': '36', '999': '37'}\n"
     ]
    }
   ],
   "source": [
    "TripType_mapper = {}\n",
    "mapper_counter = 0\n",
    "for TripType in sorted(TripType_set):\n",
    "    TripType_mapper[TripType] = str(mapper_counter)\n",
    "    mapper_counter += 1\n",
    "\n",
    "print(TripType_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have finished transactions: 0\n",
      "Have finished transactions: 5000\n",
      "Have finished transactions: 10000\n",
      "Have finished transactions: 15000\n",
      "Have finished transactions: 20000\n",
      "Have finished transactions: 25000\n",
      "Have finished transactions: 30000\n",
      "Have finished transactions: 35000\n",
      "Have finished transactions: 40000\n",
      "Have finished transactions: 45000\n",
      "Have finished transactions: 50000\n",
      "Have finished transactions: 55000\n",
      "Have finished transactions: 60000\n",
      "Have finished transactions: 65000\n",
      "Have finished transactions: 70000\n",
      "Have finished transactions: 75000\n",
      "Have finished transactions: 80000\n",
      "Have finished transactions: 85000\n",
      "Have finished transactions: 90000\n",
      "Have finished transactions: 95000\n",
      "Have finished transactions: 100000\n",
      "Have finished transactions: 105000\n",
      "Have finished transactions: 110000\n",
      "Have finished transactions: 115000\n",
      "Have finished transactions: 120000\n",
      "Have finished transactions: 125000\n",
      "Have finished transactions: 130000\n",
      "Have finished transactions: 135000\n",
      "Have finished transactions: 140000\n",
      "Have finished transactions: 145000\n",
      "Have finished transactions: 150000\n",
      "Have finished transactions: 155000\n",
      "Have finished transactions: 160000\n",
      "Have finished transactions: 165000\n",
      "Have finished transactions: 170000\n",
      "Have finished transactions: 175000\n",
      "Have finished transactions: 180000\n",
      "Have finished transactions: 185000\n",
      "Have finished transactions: 190000\n",
      "Have finished transactions: 195000\n",
      "Have finished transactions: 200000\n",
      "Have finished transactions: 205000\n",
      "Have finished transactions: 210000\n",
      "Have finished transactions: 215000\n",
      "Have finished transactions: 220000\n",
      "Have finished transactions: 225000\n",
      "Have finished transactions: 230000\n",
      "Have finished transactions: 235000\n",
      "Have finished transactions: 240000\n",
      "Have finished transactions: 245000\n",
      "Have finished transactions: 250000\n",
      "Have finished transactions: 255000\n",
      "Have finished transactions: 260000\n",
      "Have finished transactions: 265000\n",
      "Have finished transactions: 270000\n",
      "Have finished transactions: 275000\n",
      "Have finished transactions: 280000\n",
      "Have finished transactions: 285000\n",
      "Have finished transactions: 290000\n",
      "Have finished transactions: 295000\n",
      "Have finished transactions: 300000\n",
      "Have finished transactions: 305000\n",
      "Have finished transactions: 310000\n",
      "Have finished transactions: 315000\n",
      "Have finished transactions: 320000\n",
      "Have finished transactions: 325000\n",
      "Have finished transactions: 330000\n",
      "Have finished transactions: 335000\n",
      "Have finished transactions: 340000\n",
      "Have finished transactions: 345000\n",
      "Have finished transactions: 350000\n",
      "Have finished transactions: 355000\n",
      "Have finished transactions: 360000\n",
      "Have finished transactions: 365000\n",
      "Have finished transactions: 370000\n",
      "Have finished transactions: 375000\n",
      "Have finished transactions: 380000\n",
      "Have finished transactions: 385000\n",
      "Have finished transactions: 390000\n",
      "Have finished transactions: 395000\n",
      "Have finished transactions: 400000\n",
      "Have finished transactions: 405000\n",
      "Have finished transactions: 410000\n",
      "Have finished transactions: 415000\n",
      "Have finished transactions: 420000\n",
      "Have finished transactions: 425000\n",
      "Have finished transactions: 430000\n",
      "Have finished transactions: 435000\n",
      "Have finished transactions: 440000\n",
      "Have finished transactions: 445000\n",
      "Have finished transactions: 450000\n",
      "Have finished transactions: 455000\n",
      "Have finished transactions: 460000\n",
      "Have finished transactions: 465000\n",
      "Have finished transactions: 470000\n",
      "Have finished transactions: 475000\n",
      "Have finished transactions: 480000\n",
      "Have finished transactions: 485000\n",
      "Have finished transactions: 490000\n",
      "Have finished transactions: 495000\n",
      "Have finished transactions: 500000\n",
      "Have finished transactions: 505000\n",
      "Have finished transactions: 510000\n",
      "Have finished transactions: 515000\n",
      "Have finished transactions: 520000\n",
      "Have finished transactions: 525000\n",
      "Have finished transactions: 530000\n",
      "Have finished transactions: 535000\n",
      "Have finished transactions: 540000\n",
      "Have finished transactions: 545000\n",
      "Have finished transactions: 550000\n",
      "Have finished transactions: 555000\n",
      "Have finished transactions: 560000\n",
      "Have finished transactions: 565000\n",
      "Have finished transactions: 570000\n",
      "Have finished transactions: 575000\n",
      "Have finished transactions: 580000\n",
      "Have finished transactions: 585000\n",
      "Have finished transactions: 590000\n",
      "Have finished transactions: 595000\n",
      "Have finished transactions: 600000\n",
      "Have finished transactions: 605000\n",
      "Have finished transactions: 610000\n",
      "Have finished transactions: 615000\n",
      "Have finished transactions: 620000\n",
      "Have finished transactions: 625000\n",
      "Have finished transactions: 630000\n",
      "Have finished transactions: 635000\n",
      "Have finished transactions: 640000\n",
      "Have finished transactions: 645000\n",
      "Data transformation finished! Total processed records: 647055\n"
     ]
    }
   ],
   "source": [
    "output_file = 'data/transformed_train.csv'\n",
    "counter = 0\n",
    "with open(input_file, 'r') as f, open(output_file, 'w') as writter:\n",
    "    for line in f:\n",
    "        line_list = line.strip('\\n').split(',')\n",
    "        DepartmentDescription = line_list[title_layout['DepartmentDescription']].strip('\"')\n",
    "        \n",
    "        if counter % 5000 == 0:\n",
    "            print('Have finished transactions:', counter)\n",
    "        \n",
    "        output_str = ''\n",
    "        if 'TripType' not in line:\n",
    "            for field in transformed_title_list:\n",
    "                if field in [\"TripType\", \"VisitNumber\", \"Weekday\", \"Upc\"]:\n",
    "                    \n",
    "                    if field == \"Weekday\":\n",
    "                        cur_val_raw = line_list[title_layout[field]].strip('\"')\n",
    "                        cur_val = str(time.strptime(cur_val_raw, \"%A\").tm_wday)\n",
    "                    \n",
    "                    elif field == \"TripType\":\n",
    "                        cur_val_raw = line_list[title_layout[field]].strip('\"')\n",
    "                        cur_val = TripType_mapper[cur_val_raw]\n",
    "                        \n",
    "                    else:\n",
    "                        cur_val = line_list[title_layout[field]].strip('\"')\n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    DepartmentDescription = line_list[title_layout['DepartmentDescription']].strip('\"')\n",
    "                    if field == DepartmentDescription:\n",
    "                        cur_val = line_list[title_layout['ScanCount']].strip('\"')\n",
    "\n",
    "                    else:\n",
    "                        cur_val = '0'\n",
    "                \n",
    "                \n",
    "                if cur_val in [' ', '']:\n",
    "                    cur_val = '-1'\n",
    "                output_str += cur_val + ','\n",
    "        \n",
    "        else:\n",
    "            for field in transformed_title_list:\n",
    "                output_str += field + ','\n",
    "        \n",
    "        writter.write(output_str.strip(',') + '\\n')\n",
    "        counter += 1\n",
    "        \n",
    "print('Data transformation finished! Total processed records:', counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in file are: 647054\n"
     ]
    }
   ],
   "source": [
    "model_ready_file = 'data/transformed_train.csv'\n",
    "def record_counter():\n",
    "    counter = 0\n",
    "    with open(model_ready_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if \"TripType\" not in line:\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n",
    "num_records = record_counter()\n",
    "print('Total records in file are:', num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_RATIO = 0.7\n",
    "\n",
    "def gen_whole_data_time_series():\n",
    "    feature_train = []\n",
    "    target_train = [] \n",
    "\n",
    "    feature_test = []\n",
    "    target_test = []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    index = 0\n",
    "    with open(model_ready_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.strip('\\n').split(',')\n",
    "\n",
    "            if \"TripType\" not in line:\n",
    "                target = int(line_list[transformed_title_layout['TripType']].strip(''))\n",
    "                feature_list_str = line_list[1:]\n",
    "\n",
    "                if index % 5000 == 0:\n",
    "                    print('Have processed number of records:', index, 'time cost:', time.time() - start)\n",
    "                    start = time.time()\n",
    "\n",
    "                feature_list_float = []\n",
    "\n",
    "                for feature_val in feature_list_str:\n",
    "                    feature_list_float.append(float(feature_val))\n",
    "\n",
    "\n",
    "                if index <= int(float(num_records) * float(CUT_RATIO)):\n",
    "\n",
    "                    feature_train.append(feature_list_float)\n",
    "                    target_train.append(target)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    feature_test.append(feature_list_float)\n",
    "                    target_test.append(target)\n",
    "\n",
    "                index += 1\n",
    "\n",
    "    return feature_train, target_train, feature_test, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample_generator_time_series():\n",
    "    ros = RandomOverSampler(random_state = random_state)\n",
    "    feature_train, target_train, feature_test, target_test = gen_whole_data_time_series()\n",
    "    feature_train_resampled, target_train_resampled = ros.fit_resample(feature_train, target_train)\n",
    "    return feature_train_resampled, target_train_resampled, feature_test, target_test\n",
    "\n",
    "\n",
    "def under_sample_generator_time_series():\n",
    "    rus = RandomUnderSampler(random_state = random_state)\n",
    "    feature_train, target_train, feature_test, target_test = gen_whole_data_time_series()\n",
    "    feature_train_resampled, target_train_resampled = rus.fit_resample(feature_train, target_train)\n",
    "    return feature_train_resampled, target_train_resampled, feature_test, target_test\n",
    "\n",
    "\n",
    "def SMOTE_sample_generator_time_series():\n",
    "    sm = SMOTE(random_state=42)\n",
    "    feature_train, target_train, feature_test, target_test = gen_whole_data_time_series()\n",
    "    feature_train_resampled, target_train_resampled = sm.fit_resample(feature_train, target_train)\n",
    "    return feature_train_resampled, target_train_resampled, feature_test, target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is using time_series sampling mode...\n",
      "Have processed number of records: 0 time cost: 0.0005991458892822266\n",
      "Have processed number of records: 5000 time cost: 0.08219194412231445\n",
      "Have processed number of records: 10000 time cost: 0.08435416221618652\n",
      "Have processed number of records: 15000 time cost: 0.07744383811950684\n",
      "Have processed number of records: 20000 time cost: 0.07208585739135742\n",
      "Have processed number of records: 25000 time cost: 0.07713007926940918\n",
      "Have processed number of records: 30000 time cost: 0.06986689567565918\n",
      "Have processed number of records: 35000 time cost: 0.07646322250366211\n",
      "Have processed number of records: 40000 time cost: 0.07737493515014648\n",
      "Have processed number of records: 45000 time cost: 0.06839513778686523\n",
      "Have processed number of records: 50000 time cost: 0.07234406471252441\n",
      "Have processed number of records: 55000 time cost: 0.06749796867370605\n",
      "Have processed number of records: 60000 time cost: 0.07337427139282227\n",
      "Have processed number of records: 65000 time cost: 0.07452011108398438\n",
      "Have processed number of records: 70000 time cost: 0.0729520320892334\n",
      "Have processed number of records: 75000 time cost: 0.08270096778869629\n",
      "Have processed number of records: 80000 time cost: 0.06850385665893555\n",
      "Have processed number of records: 85000 time cost: 0.07657694816589355\n",
      "Have processed number of records: 90000 time cost: 0.07742476463317871\n",
      "Have processed number of records: 95000 time cost: 0.07343316078186035\n",
      "Have processed number of records: 100000 time cost: 0.07640290260314941\n",
      "Have processed number of records: 105000 time cost: 0.07227683067321777\n",
      "Have processed number of records: 110000 time cost: 0.07631468772888184\n",
      "Have processed number of records: 115000 time cost: 0.07682418823242188\n",
      "Have processed number of records: 120000 time cost: 0.09211277961730957\n",
      "Have processed number of records: 125000 time cost: 0.07863998413085938\n",
      "Have processed number of records: 130000 time cost: 0.07166314125061035\n",
      "Have processed number of records: 135000 time cost: 0.07674908638000488\n",
      "Have processed number of records: 140000 time cost: 0.06988286972045898\n",
      "Have processed number of records: 145000 time cost: 0.07587718963623047\n",
      "Have processed number of records: 150000 time cost: 0.08279085159301758\n",
      "Have processed number of records: 155000 time cost: 0.08054995536804199\n",
      "Have processed number of records: 160000 time cost: 0.08197236061096191\n",
      "Have processed number of records: 165000 time cost: 0.07563900947570801\n",
      "Have processed number of records: 170000 time cost: 0.820296049118042\n",
      "Have processed number of records: 175000 time cost: 0.07489991188049316\n",
      "Have processed number of records: 180000 time cost: 0.07232809066772461\n",
      "Have processed number of records: 185000 time cost: 0.07667207717895508\n",
      "Have processed number of records: 190000 time cost: 0.0722188949584961\n",
      "Have processed number of records: 195000 time cost: 0.08493208885192871\n",
      "Have processed number of records: 200000 time cost: 0.08431005477905273\n",
      "Have processed number of records: 205000 time cost: 0.07599425315856934\n",
      "Have processed number of records: 210000 time cost: 0.08546924591064453\n",
      "Have processed number of records: 215000 time cost: 0.07680583000183105\n",
      "Have processed number of records: 220000 time cost: 0.07988810539245605\n",
      "Have processed number of records: 225000 time cost: 0.07575821876525879\n",
      "Have processed number of records: 230000 time cost: 0.07272791862487793\n",
      "Have processed number of records: 235000 time cost: 0.07735681533813477\n",
      "Have processed number of records: 240000 time cost: 0.07200789451599121\n",
      "Have processed number of records: 245000 time cost: 0.07552003860473633\n",
      "Have processed number of records: 250000 time cost: 0.07375001907348633\n",
      "Have processed number of records: 255000 time cost: 0.07532906532287598\n",
      "Have processed number of records: 260000 time cost: 0.07705211639404297\n",
      "Have processed number of records: 265000 time cost: 0.07723402976989746\n",
      "Have processed number of records: 270000 time cost: 0.08566093444824219\n",
      "Have processed number of records: 275000 time cost: 0.0742647647857666\n",
      "Have processed number of records: 280000 time cost: 0.08305716514587402\n",
      "Have processed number of records: 285000 time cost: 0.07955408096313477\n",
      "Have processed number of records: 290000 time cost: 0.0706949234008789\n",
      "Have processed number of records: 295000 time cost: 0.07766485214233398\n",
      "Have processed number of records: 300000 time cost: 0.0732569694519043\n",
      "Have processed number of records: 305000 time cost: 0.07589125633239746\n",
      "Have processed number of records: 310000 time cost: 0.08682489395141602\n",
      "Have processed number of records: 315000 time cost: 0.07287812232971191\n",
      "Have processed number of records: 320000 time cost: 0.07306122779846191\n",
      "Have processed number of records: 325000 time cost: 0.0740358829498291\n",
      "Have processed number of records: 330000 time cost: 0.08206677436828613\n",
      "Have processed number of records: 335000 time cost: 0.08257508277893066\n",
      "Have processed number of records: 340000 time cost: 0.0733487606048584\n",
      "Have processed number of records: 345000 time cost: 0.07365131378173828\n",
      "Have processed number of records: 350000 time cost: 0.07018017768859863\n",
      "Have processed number of records: 355000 time cost: 0.07683014869689941\n",
      "Have processed number of records: 360000 time cost: 0.07245278358459473\n",
      "Have processed number of records: 365000 time cost: 0.07181000709533691\n",
      "Have processed number of records: 370000 time cost: 0.0766451358795166\n",
      "Have processed number of records: 375000 time cost: 0.07286620140075684\n",
      "Have processed number of records: 380000 time cost: 0.07749295234680176\n",
      "Have processed number of records: 385000 time cost: 0.07718491554260254\n",
      "Have processed number of records: 390000 time cost: 0.07232403755187988\n",
      "Have processed number of records: 395000 time cost: 0.07538366317749023\n",
      "Have processed number of records: 400000 time cost: 0.07169890403747559\n",
      "Have processed number of records: 405000 time cost: 1.014815092086792\n",
      "Have processed number of records: 410000 time cost: 0.07054901123046875\n",
      "Have processed number of records: 415000 time cost: 0.07382607460021973\n",
      "Have processed number of records: 420000 time cost: 0.07442188262939453\n",
      "Have processed number of records: 425000 time cost: 0.0697021484375\n",
      "Have processed number of records: 430000 time cost: 0.07397985458374023\n",
      "Have processed number of records: 435000 time cost: 0.07031965255737305\n",
      "Have processed number of records: 440000 time cost: 0.0885782241821289\n",
      "Have processed number of records: 445000 time cost: 0.08411002159118652\n",
      "Have processed number of records: 450000 time cost: 0.07225704193115234\n",
      "Have processed number of records: 455000 time cost: 0.07715392112731934\n",
      "Have processed number of records: 460000 time cost: 0.07211804389953613\n",
      "Have processed number of records: 465000 time cost: 0.08107399940490723\n",
      "Have processed number of records: 470000 time cost: 0.08206796646118164\n",
      "Have processed number of records: 475000 time cost: 0.07448601722717285\n",
      "Have processed number of records: 480000 time cost: 0.07724809646606445\n",
      "Have processed number of records: 485000 time cost: 0.07271313667297363\n",
      "Have processed number of records: 490000 time cost: 0.08577084541320801\n",
      "Have processed number of records: 495000 time cost: 0.07652878761291504\n",
      "Have processed number of records: 500000 time cost: 0.08362197875976562\n",
      "Have processed number of records: 505000 time cost: 0.08776283264160156\n",
      "Have processed number of records: 510000 time cost: 0.0758981704711914\n",
      "Have processed number of records: 515000 time cost: 0.07905697822570801\n",
      "Have processed number of records: 520000 time cost: 0.0761411190032959\n",
      "Have processed number of records: 525000 time cost: 0.07220196723937988\n",
      "Have processed number of records: 530000 time cost: 0.07624435424804688\n",
      "Have processed number of records: 535000 time cost: 0.0732278823852539\n",
      "Have processed number of records: 540000 time cost: 0.0790560245513916\n",
      "Have processed number of records: 545000 time cost: 0.07892394065856934\n",
      "Have processed number of records: 550000 time cost: 0.07788491249084473\n",
      "Have processed number of records: 555000 time cost: 0.0863490104675293\n",
      "Have processed number of records: 560000 time cost: 0.07496285438537598\n",
      "Have processed number of records: 565000 time cost: 0.07863092422485352\n",
      "Have processed number of records: 570000 time cost: 0.08081197738647461\n",
      "Have processed number of records: 575000 time cost: 0.07386088371276855\n",
      "Have processed number of records: 580000 time cost: 0.0789189338684082\n",
      "Have processed number of records: 585000 time cost: 0.07277369499206543\n",
      "Have processed number of records: 590000 time cost: 0.0798029899597168\n",
      "Have processed number of records: 595000 time cost: 0.0774691104888916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have processed number of records: 600000 time cost: 0.08636593818664551\n",
      "Have processed number of records: 605000 time cost: 0.08423781394958496\n",
      "Have processed number of records: 610000 time cost: 0.0748281478881836\n",
      "Have processed number of records: 615000 time cost: 0.08139324188232422\n",
      "Have processed number of records: 620000 time cost: 0.08652687072753906\n",
      "Have processed number of records: 625000 time cost: 0.08004999160766602\n",
      "Have processed number of records: 630000 time cost: 0.08004379272460938\n",
      "Have processed number of records: 635000 time cost: 0.07200503349304199\n",
      "Have processed number of records: 640000 time cost: 0.07936573028564453\n",
      "Have processed number of records: 645000 time cost: 0.08492064476013184\n",
      "Total time to process all data is: 12.267173051834106\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "random_state = 888\n",
    "\n",
    "sample_selection_mode = 'time_series'\n",
    "\n",
    "data_processing_start = time.time()\n",
    "if sample_selection_mode == 'time_series':\n",
    "    print('Is using time_series sampling mode...')\n",
    "    feature_train, target_train, feature_test, target_test = gen_whole_data_time_series()\n",
    "\n",
    "elif sample_selection_mode == 'time_series_over':\n",
    "    print('Is using time_series_over mode...')\n",
    "    feature_train, target_train, feature_test, target_test = over_sample_generator_time_series()\n",
    "\n",
    "elif sample_selection_mode == 'time_series_under':\n",
    "    print('Is using time_series_under mode...')\n",
    "    feature_train, target_train, feature_test, target_test = under_sample_generator_time_series()\n",
    "\n",
    "else:\n",
    "    print('Is using time_series_smote mode...')\n",
    "    feature_train, target_train, feature_test, target_test = SMOTE_sample_generator_time_series()\n",
    "\n",
    "\n",
    "print('Total time to process all data is:', time.time() - data_processing_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 452938 452938\n",
      "Length of validation dataset: 194116 194116\n"
     ]
    }
   ],
   "source": [
    "print('Length of training dataset:', len(feature_train), len(target_train))\n",
    "print('Length of validation dataset:', len(feature_test), len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number or class: 38\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]\n"
     ]
    }
   ],
   "source": [
    "num_class_set = set()\n",
    "for target in target_train:\n",
    "    num_class_set.add(target)\n",
    "\n",
    "for target in target_test:\n",
    "    num_class_set.add(target)\n",
    "\n",
    "print('Total number or class:', len(num_class_set))\n",
    "print(sorted(num_class_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 5,  # the maximum depth of each tree\n",
    "    'eta': 0.1,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training # binary:logistic #multi:softprob\n",
    "    'scale_pos_weight': 1.0,\n",
    "    'feature_selector': 'greedy',\n",
    "    'num_class': len(num_class_set),\n",
    "    'tree_method': 'hist'\n",
    "    #'grow_policy': 'lossguide', # depthwise #Controls a way new nodes are added to the tree.\n",
    "}  # the number of classes that exist in this datset #'num_class': 2\n",
    "\n",
    "num_round = 100  # the number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost model will store at: models/\n"
     ]
    }
   ],
   "source": [
    "import datetime, os\n",
    "\n",
    "DT_format = \"%Y%m%d%H%M%S\"\n",
    "work_dir = ''\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "model_version = currentDT.strftime(DT_format)\n",
    "\n",
    "text_model_store_location = work_dir + 'models/'\n",
    "if not os.path.exists(text_model_store_location):\n",
    "    os.makedirs(text_model_store_location)\n",
    "print('Xgboost model will store at:', text_model_store_location)\n",
    "model_file = text_model_store_location + 'creditcard_xgboost_model_' + model_version + '.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is training the xgboost model...\n",
      "Training process finished! Time cost: 83.1996099948883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "training_list = transformed_title_list[1:]\n",
    "def train_model_xgboost(feature_train, target_train):\n",
    "\n",
    "    dtrain = xgb.DMatrix(feature_train, label=target_train, feature_names=training_list)\n",
    "    bst = xgb.train(param, dtrain, num_round)\n",
    "    bst.dump_model(model_file)\n",
    "    return bst\n",
    "    \n",
    "print(\"Is training the xgboost model...\")\n",
    "start = time.time()\n",
    "model_bst = train_model_xgboost(feature_train, target_train)\n",
    "print(\"Training process finished! Time cost:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is inference the predicted results by using the validation data...\n",
      "Inference finished!\n"
     ]
    }
   ],
   "source": [
    "def inference_xgboost(bst, feature_test, target_test):\n",
    "    dtest = xgb.DMatrix(feature_test, label=target_test, feature_names=training_list)\n",
    "    preds = bst.predict(dtest)\n",
    "    return preds\n",
    "\n",
    "print('Is inference the predicted results by using the validation data...')\n",
    "prediction_result = inference_xgboost(model_bst, feature_test, target_test)\n",
    "print('Inference finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the feature importance list...\n",
      "Credit Card Fraud Detection Feature Importance List:\n",
      "Feature Name: PHARMACY RX \t\t Feature Abs_importance: 405.01018629797915\n",
      "Feature Name: LADIESWEAR \t\t Feature Abs_importance: 317.1432267111415\n",
      "Feature Name: INFANT CONSUMABLE HARDLINES \t\t Feature Abs_importance: 288.4132029604081\n",
      "Feature Name: FINANCIAL SERVICES \t\t Feature Abs_importance: 243.09695698210498\n",
      "Feature Name: SERVICE DELI \t\t Feature Abs_importance: 229.38511061607576\n",
      "Feature Name: INFANT APPAREL \t\t Feature Abs_importance: 227.7977676462042\n",
      "Feature Name: BEAUTY \t\t Feature Abs_importance: 186.09443863618165\n",
      "Feature Name: MENS WEAR \t\t Feature Abs_importance: 178.6209275683727\n",
      "Feature Name: PHARMACY OTC \t\t Feature Abs_importance: 167.28170031432407\n",
      "Feature Name: PERSONAL CARE \t\t Feature Abs_importance: 154.77936096611543\n",
      "Feature Name: PRODUCE \t\t Feature Abs_importance: 147.7360533181869\n",
      "Feature Name: HORTICULTURE AND ACCESS \t\t Feature Abs_importance: 147.42793405246383\n",
      "Feature Name: BRAS & SHAPEWEAR \t\t Feature Abs_importance: 147.00538978677085\n",
      "Feature Name: OPTICAL - FRAMES \t\t Feature Abs_importance: 133.63213080756475\n",
      "Feature Name: WIRELESS \t\t Feature Abs_importance: 124.64895635271276\n",
      "Feature Name: CELEBRATION \t\t Feature Abs_importance: 122.81006953177135\n",
      "Feature Name: PLUS AND MATERNITY \t\t Feature Abs_importance: 122.48867254297299\n",
      "Feature Name: HOUSEHOLD CHEMICALS/SUPP \t\t Feature Abs_importance: 122.06986930231088\n",
      "Feature Name: BEDDING \t\t Feature Abs_importance: 121.61422841556625\n",
      "Feature Name: SLEEPWEAR/FOUNDATIONS \t\t Feature Abs_importance: 120.15233109470591\n",
      "Feature Name: AUTOMOTIVE \t\t Feature Abs_importance: 113.40676644020174\n",
      "Feature Name: MEAT - FRESH & FROZEN \t\t Feature Abs_importance: 113.08966229229705\n",
      "Feature Name: GIRLS WEAR \t\t Feature Abs_importance: 112.56768993707216\n",
      "Feature Name: GROCERY DRY GOODS \t\t Feature Abs_importance: 108.61477938029446\n",
      "Feature Name: DAIRY \t\t Feature Abs_importance: 108.38317637333047\n",
      "Feature Name: COOK AND DINE \t\t Feature Abs_importance: 107.34805937164086\n",
      "Feature Name: BOYS WEAR \t\t Feature Abs_importance: 106.85579298266916\n",
      "Feature Name: HOME MANAGEMENT \t\t Feature Abs_importance: 104.04711350699641\n",
      "Feature Name: SWIMWEAR/OUTERWEAR \t\t Feature Abs_importance: 101.08855501666667\n",
      "Feature Name: DSD GROCERY \t\t Feature Abs_importance: 99.33743472318\n",
      "Feature Name: LAWN AND GARDEN \t\t Feature Abs_importance: 98.9646296979676\n",
      "Feature Name: SHOES \t\t Feature Abs_importance: 97.5926027645016\n",
      "Feature Name: SPORTING GOODS \t\t Feature Abs_importance: 94.19994834473326\n",
      "Feature Name: LIQUOR \t\t Feature Abs_importance: 90.03740710011385\n",
      "Feature Name: JEWELRY AND SUNGLASSES \t\t Feature Abs_importance: 87.69141763897186\n",
      "Feature Name: FURNITURE \t\t Feature Abs_importance: 85.99374368264614\n",
      "Feature Name: BATH AND SHOWER \t\t Feature Abs_importance: 84.35435079605628\n",
      "Feature Name: ELECTRONICS \t\t Feature Abs_importance: 84.28589265121731\n",
      "Feature Name: PETS AND SUPPLIES \t\t Feature Abs_importance: 83.21532411864759\n",
      "Feature Name: OTHER DEPARTMENTS \t\t Feature Abs_importance: 82.1277237\n",
      "Feature Name: HOME DECOR \t\t Feature Abs_importance: 78.68040233624525\n",
      "Feature Name: OFFICE SUPPLIES \t\t Feature Abs_importance: 78.28290400772306\n",
      "Feature Name: TOYS \t\t Feature Abs_importance: 74.63204007542905\n",
      "Feature Name: FROZEN FOODS \t\t Feature Abs_importance: 70.91252670437818\n",
      "Feature Name: 1-HR PHOTO \t\t Feature Abs_importance: 68.09117218954545\n",
      "Feature Name: FABRICS AND CRAFTS \t\t Feature Abs_importance: 66.63192969502553\n",
      "Feature Name: MENSWEAR \t\t Feature Abs_importance: 58.26519325294118\n",
      "Feature Name: OPTICAL - LENSES \t\t Feature Abs_importance: 56.83659341090908\n",
      "Feature Name: LARGE HOUSEHOLD GOODS \t\t Feature Abs_importance: 56.43397174727272\n",
      "Feature Name: HARDWARE \t\t Feature Abs_importance: 55.26348560645488\n",
      "Feature Name: COMM BREAD \t\t Feature Abs_importance: 55.1970388668483\n",
      "Feature Name: HOUSEHOLD PAPER GOODS \t\t Feature Abs_importance: 46.34279803375349\n",
      "Feature Name: PRE PACKED DELI \t\t Feature Abs_importance: 43.57111931041225\n",
      "Feature Name: MEDIA AND GAMING \t\t Feature Abs_importance: 42.381817675512096\n",
      "Feature Name: IMPULSE MERCHANDISE \t\t Feature Abs_importance: 41.52593563682786\n",
      "Feature Name: CANDY \t\t Feature Abs_importance: 41.39934789232699\n",
      "Feature Name: ACCESSORIES \t\t Feature Abs_importance: 41.34368505000001\n",
      "Feature Name: PAINT AND ACCESSORIES \t\t Feature Abs_importance: 40.932059788586535\n",
      "Feature Name: SEAFOOD \t\t Feature Abs_importance: 40.68515032471761\n",
      "Feature Name: LADIES SOCKS \t\t Feature Abs_importance: 34.30162827781818\n",
      "Feature Name: NULL \t\t Feature Abs_importance: 33.85768089786363\n",
      "Feature Name: CAMERAS AND SUPPLIES \t\t Feature Abs_importance: 33.687576204491236\n",
      "Feature Name: CONCEPT STORES \t\t Feature Abs_importance: 32.806063054999996\n",
      "Feature Name: BAKERY \t\t Feature Abs_importance: 31.72727551718662\n",
      "Feature Name: BOOKS AND MAGAZINES \t\t Feature Abs_importance: 28.061739307671235\n",
      "Feature Name: PLAYERS AND ELECTRONICS \t\t Feature Abs_importance: 21.94491761606225\n",
      "Feature Name: SHEER HOSIERY \t\t Feature Abs_importance: 21.519642862270832\n",
      "Feature Name: Upc \t\t Feature Abs_importance: 12.717479934217714\n",
      "Feature Name: Weekday \t\t Feature Abs_importance: 5.303455763360588\n",
      "Feature Name: VisitNumber \t\t Feature Abs_importance: 4.498914767209569\n"
     ]
    }
   ],
   "source": [
    "def print_feature_importance(importance_dict):\n",
    "    sorted_importance_dict = sorted(importance_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    print('Credit Card Fraud Detection Feature Importance List:')\n",
    "    for item in sorted_importance_dict:\n",
    "        print('Feature Name:', item[0], '\\t\\t', 'Feature Abs_importance:', item[1])\n",
    "            \n",
    "print(\"Generating the feature importance list...\")\n",
    "importance_dict = model_bst.get_score(importance_type='gain')\n",
    "print_feature_importance(importance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0175093  0.00037399 0.00829174 ... 0.00599242 0.03432281 0.09758631]\n",
      " [0.04437825 0.00014111 0.00599576 ... 0.02677289 0.02879497 0.01777044]\n",
      " [0.0203148  0.0001539  0.00690398 ... 0.02377742 0.09489341 0.01938147]\n",
      " ...\n",
      " [0.02044101 0.00013065 0.00620467 ... 0.01976452 0.00484215 0.00723294]\n",
      " [0.02446288 0.00015465 0.00492902 ... 0.02552869 0.00699225 0.01599202]\n",
      " [0.03645201 0.0001242  0.00499405 ... 0.01808716 0.00323449 0.0050182 ]]\n",
      "Subset accuracy: 0.3441601928743638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "#print(len(target_test))\n",
    "#print(len(prediction_result))\n",
    "\n",
    "print(prediction_result)\n",
    "ylabel = np.argmax(prediction_result, axis=1)\n",
    "\n",
    "yy_list = []\n",
    "for item in ylabel:\n",
    "    yy_list.append(item)\n",
    "\n",
    "accuracy_1 = accuracy_score(np.array(target_test), np.array(yy_list))\n",
    "accuracy_2 = accuracy_score(np.array(target_test), np.array(yy_list), normalize=True, sample_weight=None)\n",
    "\n",
    "print('Subset accuracy:', accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
